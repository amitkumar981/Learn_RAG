{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitkumar981/Learn_RAG/blob/master/autonomous_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab5f0ce8",
      "metadata": {
        "id": "ab5f0ce8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(override=True)\n",
        "\n",
        "os.environ['OPENAI_API_KEY']=os.getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d835605",
      "metadata": {
        "id": "5d835605",
        "outputId": "9e1e15da-bbd5-4954-c9fd-88c7d65ef5b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatOpenAI(profile={'max_input_tokens': 128000, 'max_output_tokens': 16384, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x0000024B611BB610>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000024B611BBC50>, root_client=<openai.OpenAI object at 0x0000024B6159EF10>, root_async_client=<openai.AsyncOpenAI object at 0x0000024B6159E250>, model_name='gpt-4o-mini', temperature=0.2, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
            ]
          },
          "execution_count": 293,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#configure the llm\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "llm = init_chat_model(model = 'gpt-4o-mini',temperature=0.2)\n",
        "llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e733049",
      "metadata": {
        "id": "7e733049",
        "outputId": "c9eaccfc-856b-4172-e0e7-6695bcab2a68"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 9, 'total_tokens': 18, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-CrKo8hXZOC4JhD9PwbvMV56jVfuVu', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b5f1e-a9c5-71d0-b631-5df698d3f1ed-0', usage_metadata={'input_tokens': 9, 'output_tokens': 9, 'total_tokens': 18, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 294,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.invoke('hii')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f721aa1",
      "metadata": {
        "id": "0f721aa1"
      },
      "outputs": [],
      "source": [
        "from typing import List, TypedDict, Optional\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "class RAGState(TypedDict):\n",
        "    original_query: str\n",
        "    optimized_query: Optional[str]\n",
        "    sub_queries: List[str]\n",
        "    retrieved_docs: List[Document]\n",
        "    answer: str\n",
        "    grade: Optional[str]\n",
        "    retry_count: int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb5b40c8",
      "metadata": {
        "id": "eb5b40c8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96b4c565",
      "metadata": {
        "id": "96b4c565",
        "outputId": "c33ea318-b2ba-431d-c4dd-6a783d7425d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/introduction/', 'title': 'Redirecting...', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n')],\n",
              " [Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/workflows/', 'title': 'Redirecting...', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n')],\n",
              " [Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/map-reduce/', 'title': 'Redirecting...', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n')],\n",
              " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a RAG agent with LangChain - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentVoice agentMulti-agentLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and GenerationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy page\\u200bOverview\\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n\\u200bConcepts\\nWe will cover the following concepts:\\n\\n\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\\n\\n\\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\\n\\n\\nOnce weâ€™ve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.If your data is already available for search (i.e., you have a function to execute a search), or youâ€™re comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\\n\\u200bPreview\\nIn this guide weâ€™ll build an app that answers questions about the websiteâ€™s content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\\nExpand for full code snippetCopyimport bs4\\nfrom langchain.agents import AgentState, create_agent\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain.messages import MessageLikeRepresentation\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\n# Load and chunk contents of the blog\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs=dict(\\n        parse_only=bs4.SoupStrainer(\\n            class_=(\"post-content\", \"post-title\", \"post-header\")\\n        )\\n    ),\\n)\\ndocs = loader.load()\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\nall_splits = text_splitter.split_documents(docs)\\n\\n# Index chunks\\n_ = vector_store.add_documents(documents=all_splits)\\n\\n# Construct a tool for retrieving context\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\\n Call ID: call_xTkJr8njRY0geNz43ZvGkX0R\\n  Args:\\n    query: task decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done by...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nTask decomposition refers to...\\nCheck out the LangSmith trace.\\n\\u200bSetup\\n\\u200bInstallation\\nThis tutorial requires these langchain dependencies:\\npipuvCopypip install langchain langchain-text-splitters langchain-community bs4\\n\\nFor more details, see our Installation guide.\\n\\u200bLangSmith\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\nOr, set them in Python:\\nCopyimport getpass\\nimport os\\n\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\n\\n\\u200bComponents\\nWe will need to select three components from LangChainâ€™s suite of integrations.\\nSelect a chat model:\\n OpenAI Anthropic Azure Google Gemini AWS Bedrock HuggingFaceðŸ‘‰ Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\nðŸ‘‰ Read the Anthropic chat model integration docsCopypip install -U \"langchain[anthropic]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"claude-sonnet-4-5-20250929\")\\nðŸ‘‰ Read the Azure chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\\n\\nmodel = init_chat_model(\\n    \"azure_openai:gpt-4.1\",\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n)\\nðŸ‘‰ Read the Google GenAI chat model integration docsCopypip install -U \"langchain[google-genai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\\n\\nmodel = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\\nðŸ‘‰ Read the AWS Bedrock chat model integration docsCopypip install -U \"langchain[aws]\"\\ninit_chat_modelModel ClassCopyfrom langchain.chat_models import init_chat_model\\n\\n# Follow the steps here to configure your credentials:\\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\\n\\nmodel = init_chat_model(\\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\\n    model_provider=\"bedrock_converse\",\\n)\\nðŸ‘‰ Read the HuggingFace chat model integration docsCopypip install -U \"langchain[huggingface]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\\n\\nmodel = init_chat_model(\\n    \"microsoft/Phi-3-mini-4k-instruct\",\\n    model_provider=\"huggingface\",\\n    temperature=0.7,\\n    max_tokens=1024,\\n)\\n\\nSelect an embeddings model:\\n OpenAI Azure Google Gemini Google Vertex AWS HuggingFace Ollama Cohere MistralAI Nomic NVIDIA Voyage AI IBM watsonx Fake IsaacusCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\\n\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\nCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\\n    os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for Azure: \")\\n\\nfrom langchain_openai import AzureOpenAIEmbeddings\\n\\nembeddings = AzureOpenAIEmbeddings(\\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\\n)\\nCopypip install -qU langchain-google-genai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"GOOGLE_API_KEY\"):\\n    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\\n\\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\\n\\nembeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\\nCopypip install -qU langchain-google-vertexai\\nCopyfrom langchain_google_vertexai import VertexAIEmbeddings\\n\\nembeddings = VertexAIEmbeddings(model=\"text-embedding-005\")\\nCopypip install -qU langchain-aws\\nCopyfrom langchain_aws import BedrockEmbeddings\\n\\nembeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\")\\nCopypip install -qU langchain-huggingface\\nCopyfrom langchain_huggingface import HuggingFaceEmbeddings\\n\\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\\nCopypip install -qU langchain-ollama\\nCopyfrom langchain_ollama import OllamaEmbeddings\\n\\nembeddings = OllamaEmbeddings(model=\"llama3\")\\nCopypip install -qU langchain-cohere\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"COHERE_API_KEY\"):\\n    os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter API key for Cohere: \")\\n\\nfrom langchain_cohere import CohereEmbeddings\\n\\nembeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\\nCopypip install -qU langchain-mistralai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"MISTRALAI_API_KEY\"):\\n    os.environ[\"MISTRALAI_API_KEY\"] = getpass.getpass(\"Enter API key for MistralAI: \")\\n\\nfrom langchain_mistralai import MistralAIEmbeddings\\n\\nembeddings = MistralAIEmbeddings(model=\"mistral-embed\")\\nCopypip install -qU langchain-nomic\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"NOMIC_API_KEY\"):\\n    os.environ[\"NOMIC_API_KEY\"] = getpass.getpass(\"Enter API key for Nomic: \")\\n\\nfrom langchain_nomic import NomicEmbeddings\\n\\nembeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\\nCopypip install -qU langchain-nvidia-ai-endpoints\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"NVIDIA_API_KEY\"):\\n    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")\\n\\nfrom langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\\n\\nembeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\")\\nCopypip install -qU langchain-voyageai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"VOYAGE_API_KEY\"):\\n    os.environ[\"VOYAGE_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\\n\\nfrom langchain-voyageai import VoyageAIEmbeddings\\n\\nembeddings = VoyageAIEmbeddings(model=\"voyage-3\")\\nCopypip install -qU langchain-ibm\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"WATSONX_APIKEY\"):\\n    os.environ[\"WATSONX_APIKEY\"] = getpass.getpass(\"Enter API key for IBM watsonx: \")\\n\\nfrom langchain_ibm import WatsonxEmbeddings\\n\\nembeddings = WatsonxEmbeddings(\\n    model_id=\"ibm/slate-125m-english-rtrvr\",\\n    url=\"https://us-south.ml.cloud.ibm.com\",\\n    project_id=\"<WATSONX PROJECT_ID>\",\\n)\\nCopypip install -qU langchain-core\\nCopyfrom langchain_core.embeddings import DeterministicFakeEmbedding\\n\\nembeddings = DeterministicFakeEmbedding(size=4096)\\nCopypip install -qU langchain-isaacus\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"ISAACUS_API_KEY\"):\\nos.environ[\"ISAACUS_API_KEY\"] = getpass.getpass(\"Enter API key for Isaacus: \")\\n\\nfrom langchain_isaacus import IsaacusEmbeddings\\n\\nembeddings = IsaacusEmbeddings(model=\"kanon-2-embedder\")\\n\\nSelect a vector store:\\n In-memory Amazon OpenSearch AstraDB Chroma FAISS Milvus MongoDB PGVector PGVectorStore Pinecone QdrantCopypip install -U \"langchain-core\"\\nCopyfrom langchain_core.vectorstores import InMemoryVectorStore\\n\\nvector_store = InMemoryVectorStore(embeddings)\\nCopypip install -qU  boto3\\nCopyfrom opensearchpy import RequestsHttpConnection\\n\\nservice = \"es\"  # must set the service as \\'es\\'\\nregion = \"us-east-2\"\\ncredentials = boto3.Session(\\n    aws_access_key_id=\"xxxxxx\", aws_secret_access_key=\"xxxxx\"\\n).get_credentials()\\nawsauth = AWS4Auth(\"xxxxx\", \"xxxxxx\", region, service, session_token=credentials.token)\\n\\nvector_store = OpenSearchVectorSearch.from_documents(\\n    docs,\\n    embeddings,\\n    opensearch_url=\"host url\",\\n    http_auth=awsauth,\\n    timeout=300,\\n    use_ssl=True,\\n    verify_certs=True,\\n    connection_class=RequestsHttpConnection,\\n    index_name=\"test-index\",\\n)\\nCopypip install -U \"langchain-astradb\"\\nCopyfrom langchain_astradb import AstraDBVectorStore\\n\\nvector_store = AstraDBVectorStore(\\n    embedding=embeddings,\\n    api_endpoint=ASTRA_DB_API_ENDPOINT,\\n    collection_name=\"astra_vector_langchain\",\\n    token=ASTRA_DB_APPLICATION_TOKEN,\\n    namespace=ASTRA_DB_NAMESPACE,\\n)\\nCopypip install -qU langchain-chroma\\nCopyfrom langchain_chroma import Chroma\\n\\nvector_store = Chroma(\\n    collection_name=\"example_collection\",\\n    embedding_function=embeddings,\\n    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\\n)\\nCopypip install -qU langchain-community faiss-cpu\\nCopyimport faiss\\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\\nfrom langchain_community.vectorstores import FAISS\\n\\nembedding_dim = len(embeddings.embed_query(\"hello world\"))\\nindex = faiss.IndexFlatL2(embedding_dim)\\n\\nvector_store = FAISS(\\n    embedding_function=embeddings,\\n    index=index,\\n    docstore=InMemoryDocstore(),\\n    index_to_docstore_id={},\\n)\\nCopypip install -qU langchain-milvus\\nCopyfrom langchain_milvus import Milvus\\n\\nURI = \"./milvus_example.db\"\\n\\nvector_store = Milvus(\\n    embedding_function=embeddings,\\n    connection_args={\"uri\": URI},\\n    index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\\n)\\nCopypip install -qU langchain-mongodb\\nCopyfrom langchain_mongodb import MongoDBAtlasVectorSearch\\n\\nvector_store = MongoDBAtlasVectorSearch(\\n    embedding=embeddings,\\n    collection=MONGODB_COLLECTION,\\n    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\\n    relevance_score_fn=\"cosine\",\\n)\\nCopypip install -qU langchain-postgres\\nCopyfrom langchain_postgres import PGVector\\n\\nvector_store = PGVector(\\n    embeddings=embeddings,\\n    collection_name=\"my_docs\",\\n    connection=\"postgresql+psycopg://...\",\\n)\\nCopypip install -qU langchain-postgres\\nCopyfrom langchain_postgres import PGEngine, PGVectorStore\\n\\npg_engine = PGEngine.from_connection_string(\\n    url=\"postgresql+psycopg://...\"\\n)\\n\\nvector_store = PGVectorStore.create_sync(\\n    engine=pg_engine,\\n    table_name=\\'test_table\\',\\n    embedding_service=embedding\\n)\\nCopypip install -qU langchain-pinecone\\nCopyfrom langchain_pinecone import PineconeVectorStore\\nfrom pinecone import Pinecone\\n\\npc = Pinecone(api_key=...)\\nindex = pc.Index(index_name)\\n\\nvector_store = PineconeVectorStore(embedding=embeddings, index=index)\\nCopypip install -qU langchain-qdrant\\nCopyfrom qdrant_client.models import Distance, VectorParams\\nfrom langchain_qdrant import QdrantVectorStore\\nfrom qdrant_client import QdrantClient\\n\\nclient = QdrantClient(\":memory:\")\\n\\nvector_size = len(embeddings.embed_query(\"sample text\"))\\n\\nif not client.collection_exists(\"test\"):\\n    client.create_collection(\\n        collection_name=\"test\",\\n        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\\n    )\\nvector_store = QdrantVectorStore(\\n    client=client,\\n    collection_name=\"test\",\\n    embedding=embeddings,\\n)\\n\\n\\u200b1. Indexing\\nThis section is an abbreviated version of the content in the semantic search tutorial.If your data is already indexed and available for search (i.e., you have a function to execute a search), or if youâ€™re comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation.\\nIndexing commonly works as follows:\\n\\nLoad: First we need to load our data. This is done with Document Loaders.\\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and wonâ€™t fit in a modelâ€™s finite context window.\\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\\n\\n\\n\\u200bLoading documents\\nWe need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects.\\nIn this case weâ€™ll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class â€œpost-contentâ€, â€œpost-titleâ€, or â€œpost-headerâ€ are relevant, so weâ€™ll remove all others.\\nCopyimport bs4\\nfrom langchain_community.document_loaders import WebBaseLoader\\n\\n# Only keep post title, headers, and content from the full HTML.\\nbs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs={\"parse_only\": bs4_strainer},\\n)\\ndocs = loader.load()\\n\\nassert len(docs) == 1\\nprint(f\"Total characters: {len(docs[0].page_content)}\")\\n\\nCopyTotal characters: 43131\\n\\nCopyprint(docs[0].page_content[:500])\\n\\nCopy      LLM Powered Autonomous Agents\\n\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn\\n\\nGo deeper\\nDocumentLoader: Object that loads data from a source as list of Documents.\\n\\nIntegrations: 160+ integrations to choose from.\\nBaseLoader: API reference for the base interface.\\n\\n\\u200bSplitting documents\\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\\nTo handle this weâ€™ll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\\nAs in the semantic search tutorial, we use a RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\\nCopyfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=1000,  # chunk size (characters)\\n    chunk_overlap=200,  # chunk overlap (characters)\\n    add_start_index=True,  # track index in original document\\n)\\nall_splits = text_splitter.split_documents(docs)\\n\\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\\n\\nCopySplit blog post into 66 sub-documents.\\n\\nGo deeper\\nTextSplitter: Object that splits a list of Document objects into smaller\\nchunks for storage and retrieval.\\n\\nIntegrations\\nInterface: API reference for the base interface.\\n\\n\\u200bStoring documents\\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.\\nCopydocument_ids = vector_store.add_documents(documents=all_splits)\\n\\nprint(document_ids[:3])\\n\\nCopy[\\'07c18af6-ad58-479a-bfb1-d508033f9c64\\', \\'9000bf8e-1993-446f-8d4d-f4e507ba4b8f\\', \\'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6\\']\\n\\nGo deeper\\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings.\\n\\nIntegrations: 30+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nVectorStore: Wrapper around a vector database, used for storing and querying embeddings.\\n\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nThis completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\\n\\u200b2. Retrieval and Generation\\nRAG applications commonly work as follows:\\n\\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\nGenerate: A model produces an answer using a prompt that includes both the question with the retrieved data\\n\\n\\nNow letâ€™s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\\nWe will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n\\u200bRAG agents\\nOne formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:\\nCopyfrom langchain.tools import tool\\n\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\nHere we use the tool decorator to configure the tool to attach raw documents as artifacts to each ToolMessage. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\\nRetrieval tools are not limited to a single string query argument, as in the above example. You can\\nforce the LLM to specify additional search parameters by adding argumentsâ€” for example, a category:Copyfrom typing import Literal\\n\\ndef retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\\n\\nGiven our tool, we can construct the agent:\\nCopyfrom langchain.agents import create_agent\\n\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\n\\nLetâ€™s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\\nCopyquery = (\\n    \"What is the standard method for Task Decomposition?\\\\n\\\\n\"\\n    \"Once you get the answer, look up common extensions of that method.\"\\n)\\n\\nfor event in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    event[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is the standard method for Task Decomposition?\\n\\nOnce you get the answer, look up common extensions of that method.\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\\n Call ID: call_d6AVxICMPQYwAKj9lgH4E337\\n  Args:\\n    query: standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\\n Call ID: call_0dbMOw7266jvETbXWn4JqWpR\\n  Args:\\n    query: common extensions of the standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\\n\\nNote that the agent:\\n\\nGenerates a query to search for a standard method for task decomposition;\\nReceiving the answer, generates a second query to search for common extensions of it;\\nHaving received all necessary context, answers the question.\\n\\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\\nYou can add a deeper level of control and customization using the LangGraph framework directlyâ€” for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraphâ€™s Agentic RAG tutorial for more advanced formulations.\\n\\u200bRAG chains\\nIn the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\\nâœ… Benefitsâš ï¸ DrawbacksSearch only when needed â€“ The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.Two inference calls â€“ When a search is performed, it requires one call to generate the query and another to produce the final response.Contextual search queries â€“ By treating search as a tool with a query input, the LLM crafts its own queries that incorporate conversational context.Reduced control â€“ The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.Multiple searches allowed â€“ The LLM can execute several searches in support of a single user query.\\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\\nIn this approach we no longer call the model in a loop, but instead make a single pass.\\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\\nCopyfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\\n\\n@dynamic_prompt\\ndef prompt_with_context(request: ModelRequest) -> str:\\n    \"\"\"Inject context into state messages.\"\"\"\\n    last_query = request.state[\"messages\"][-1].text\\n    retrieved_docs = vector_store.similarity_search(last_query)\\n\\n    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n    system_message = (\\n        \"You are a helpful assistant. Use the following context in your response:\"\\n        f\"\\\\n\\\\n{docs_content}\"\\n    )\\n\\n    return system_message\\n\\n\\nagent = create_agent(model, tools=[], middleware=[prompt_with_context])\\n\\nLetâ€™s try this out:\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\n\\nTask decomposition is...\\n\\nIn the LangSmith trace we can see the retrieved context incorporated into the model prompt.\\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\\nReturning source documentsThe above RAG chain incorporates retrieved context into a single system message for that run.As in the agentic RAG formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\\nAdding a key to the state to store the retrieved documents\\nAdding a new node via a pre-model hook to populate that key (as well as inject the context).\\nCopyfrom typing import Any\\nfrom langchain_core.documents import Document\\nfrom langchain.agents.middleware import AgentMiddleware, AgentState\\n\\n\\nclass State(AgentState):\\n    context: list[Document]\\n\\n\\nclass RetrieveDocumentsMiddleware(AgentMiddleware[State]):\\n    state_schema = State\\n\\n    def before_model(self, state: AgentState) -> dict[str, Any] | None:\\n        last_message = state[\"messages\"][-1]\\n        retrieved_docs = vector_store.similarity_search(last_message.text)\\n\\n        docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n        augmented_message_content = (\\n            f\"{last_message.text}\\\\n\\\\n\"\\n            \"Use the following context to answer the query:\\\\n\"\\n            f\"{docs_content}\"\\n        )\\n        return {\\n            \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\\n            \"context\": retrieved_docs,\\n        }\\n\\n\\nagent = create_agent(\\n    model,\\n    tools=[],\\n    middleware=[RetrieveDocumentsMiddleware()],\\n)\\n\\n\\u200bNext steps\\nNow that weâ€™ve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:\\n\\nStream tokens and other information for responsive user experiences\\nAdd conversational memory to support multi-turn interactions\\nAdd long-term memory to support memory across conversational threads\\nAdd structured responses\\nDeploy your application with LangSmith Deployment\\n\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoBuild a semantic search engine with LangChainPreviousBuild a SQL agentNextâŒ˜IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')],\n",
              " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a RAG agent with LangChain - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentVoice agentMulti-agentLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and GenerationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy page\\u200bOverview\\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n\\u200bConcepts\\nWe will cover the following concepts:\\n\\n\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\\n\\n\\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\\n\\n\\nOnce weâ€™ve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.If your data is already available for search (i.e., you have a function to execute a search), or youâ€™re comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\\n\\u200bPreview\\nIn this guide weâ€™ll build an app that answers questions about the websiteâ€™s content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\\nExpand for full code snippetCopyimport bs4\\nfrom langchain.agents import AgentState, create_agent\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain.messages import MessageLikeRepresentation\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\n# Load and chunk contents of the blog\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs=dict(\\n        parse_only=bs4.SoupStrainer(\\n            class_=(\"post-content\", \"post-title\", \"post-header\")\\n        )\\n    ),\\n)\\ndocs = loader.load()\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\nall_splits = text_splitter.split_documents(docs)\\n\\n# Index chunks\\n_ = vector_store.add_documents(documents=all_splits)\\n\\n# Construct a tool for retrieving context\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\\n Call ID: call_xTkJr8njRY0geNz43ZvGkX0R\\n  Args:\\n    query: task decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done by...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nTask decomposition refers to...\\nCheck out the LangSmith trace.\\n\\u200bSetup\\n\\u200bInstallation\\nThis tutorial requires these langchain dependencies:\\npipuvCopypip install langchain langchain-text-splitters langchain-community bs4\\n\\nFor more details, see our Installation guide.\\n\\u200bLangSmith\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\nOr, set them in Python:\\nCopyimport getpass\\nimport os\\n\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\n\\n\\u200bComponents\\nWe will need to select three components from LangChainâ€™s suite of integrations.\\nSelect a chat model:\\n OpenAI Anthropic Azure Google Gemini AWS Bedrock HuggingFaceðŸ‘‰ Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\nðŸ‘‰ Read the Anthropic chat model integration docsCopypip install -U \"langchain[anthropic]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"claude-sonnet-4-5-20250929\")\\nðŸ‘‰ Read the Azure chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\\n\\nmodel = init_chat_model(\\n    \"azure_openai:gpt-4.1\",\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n)\\nðŸ‘‰ Read the Google GenAI chat model integration docsCopypip install -U \"langchain[google-genai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\\n\\nmodel = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\\nðŸ‘‰ Read the AWS Bedrock chat model integration docsCopypip install -U \"langchain[aws]\"\\ninit_chat_modelModel ClassCopyfrom langchain.chat_models import init_chat_model\\n\\n# Follow the steps here to configure your credentials:\\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\\n\\nmodel = init_chat_model(\\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\\n    model_provider=\"bedrock_converse\",\\n)\\nðŸ‘‰ Read the HuggingFace chat model integration docsCopypip install -U \"langchain[huggingface]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\\n\\nmodel = init_chat_model(\\n    \"microsoft/Phi-3-mini-4k-instruct\",\\n    model_provider=\"huggingface\",\\n    temperature=0.7,\\n    max_tokens=1024,\\n)\\n\\nSelect an embeddings model:\\n OpenAI Azure Google Gemini Google Vertex AWS HuggingFace Ollama Cohere MistralAI Nomic NVIDIA Voyage AI IBM watsonx Fake IsaacusCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\\n\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\nCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\\n    os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for Azure: \")\\n\\nfrom langchain_openai import AzureOpenAIEmbeddings\\n\\nembeddings = AzureOpenAIEmbeddings(\\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\\n)\\nCopypip install -qU langchain-google-genai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"GOOGLE_API_KEY\"):\\n    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\\n\\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\\n\\nembeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\\nCopypip install -qU langchain-google-vertexai\\nCopyfrom langchain_google_vertexai import VertexAIEmbeddings\\n\\nembeddings = VertexAIEmbeddings(model=\"text-embedding-005\")\\nCopypip install -qU langchain-aws\\nCopyfrom langchain_aws import BedrockEmbeddings\\n\\nembeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\")\\nCopypip install -qU langchain-huggingface\\nCopyfrom langchain_huggingface import HuggingFaceEmbeddings\\n\\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\\nCopypip install -qU langchain-ollama\\nCopyfrom langchain_ollama import OllamaEmbeddings\\n\\nembeddings = OllamaEmbeddings(model=\"llama3\")\\nCopypip install -qU langchain-cohere\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"COHERE_API_KEY\"):\\n    os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter API key for Cohere: \")\\n\\nfrom langchain_cohere import CohereEmbeddings\\n\\nembeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\\nCopypip install -qU langchain-mistralai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"MISTRALAI_API_KEY\"):\\n    os.environ[\"MISTRALAI_API_KEY\"] = getpass.getpass(\"Enter API key for MistralAI: \")\\n\\nfrom langchain_mistralai import MistralAIEmbeddings\\n\\nembeddings = MistralAIEmbeddings(model=\"mistral-embed\")\\nCopypip install -qU langchain-nomic\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"NOMIC_API_KEY\"):\\n    os.environ[\"NOMIC_API_KEY\"] = getpass.getpass(\"Enter API key for Nomic: \")\\n\\nfrom langchain_nomic import NomicEmbeddings\\n\\nembeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\\nCopypip install -qU langchain-nvidia-ai-endpoints\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"NVIDIA_API_KEY\"):\\n    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")\\n\\nfrom langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\\n\\nembeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\")\\nCopypip install -qU langchain-voyageai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"VOYAGE_API_KEY\"):\\n    os.environ[\"VOYAGE_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\\n\\nfrom langchain-voyageai import VoyageAIEmbeddings\\n\\nembeddings = VoyageAIEmbeddings(model=\"voyage-3\")\\nCopypip install -qU langchain-ibm\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"WATSONX_APIKEY\"):\\n    os.environ[\"WATSONX_APIKEY\"] = getpass.getpass(\"Enter API key for IBM watsonx: \")\\n\\nfrom langchain_ibm import WatsonxEmbeddings\\n\\nembeddings = WatsonxEmbeddings(\\n    model_id=\"ibm/slate-125m-english-rtrvr\",\\n    url=\"https://us-south.ml.cloud.ibm.com\",\\n    project_id=\"<WATSONX PROJECT_ID>\",\\n)\\nCopypip install -qU langchain-core\\nCopyfrom langchain_core.embeddings import DeterministicFakeEmbedding\\n\\nembeddings = DeterministicFakeEmbedding(size=4096)\\nCopypip install -qU langchain-isaacus\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"ISAACUS_API_KEY\"):\\nos.environ[\"ISAACUS_API_KEY\"] = getpass.getpass(\"Enter API key for Isaacus: \")\\n\\nfrom langchain_isaacus import IsaacusEmbeddings\\n\\nembeddings = IsaacusEmbeddings(model=\"kanon-2-embedder\")\\n\\nSelect a vector store:\\n In-memory Amazon OpenSearch AstraDB Chroma FAISS Milvus MongoDB PGVector PGVectorStore Pinecone QdrantCopypip install -U \"langchain-core\"\\nCopyfrom langchain_core.vectorstores import InMemoryVectorStore\\n\\nvector_store = InMemoryVectorStore(embeddings)\\nCopypip install -qU  boto3\\nCopyfrom opensearchpy import RequestsHttpConnection\\n\\nservice = \"es\"  # must set the service as \\'es\\'\\nregion = \"us-east-2\"\\ncredentials = boto3.Session(\\n    aws_access_key_id=\"xxxxxx\", aws_secret_access_key=\"xxxxx\"\\n).get_credentials()\\nawsauth = AWS4Auth(\"xxxxx\", \"xxxxxx\", region, service, session_token=credentials.token)\\n\\nvector_store = OpenSearchVectorSearch.from_documents(\\n    docs,\\n    embeddings,\\n    opensearch_url=\"host url\",\\n    http_auth=awsauth,\\n    timeout=300,\\n    use_ssl=True,\\n    verify_certs=True,\\n    connection_class=RequestsHttpConnection,\\n    index_name=\"test-index\",\\n)\\nCopypip install -U \"langchain-astradb\"\\nCopyfrom langchain_astradb import AstraDBVectorStore\\n\\nvector_store = AstraDBVectorStore(\\n    embedding=embeddings,\\n    api_endpoint=ASTRA_DB_API_ENDPOINT,\\n    collection_name=\"astra_vector_langchain\",\\n    token=ASTRA_DB_APPLICATION_TOKEN,\\n    namespace=ASTRA_DB_NAMESPACE,\\n)\\nCopypip install -qU langchain-chroma\\nCopyfrom langchain_chroma import Chroma\\n\\nvector_store = Chroma(\\n    collection_name=\"example_collection\",\\n    embedding_function=embeddings,\\n    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\\n)\\nCopypip install -qU langchain-community faiss-cpu\\nCopyimport faiss\\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\\nfrom langchain_community.vectorstores import FAISS\\n\\nembedding_dim = len(embeddings.embed_query(\"hello world\"))\\nindex = faiss.IndexFlatL2(embedding_dim)\\n\\nvector_store = FAISS(\\n    embedding_function=embeddings,\\n    index=index,\\n    docstore=InMemoryDocstore(),\\n    index_to_docstore_id={},\\n)\\nCopypip install -qU langchain-milvus\\nCopyfrom langchain_milvus import Milvus\\n\\nURI = \"./milvus_example.db\"\\n\\nvector_store = Milvus(\\n    embedding_function=embeddings,\\n    connection_args={\"uri\": URI},\\n    index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\\n)\\nCopypip install -qU langchain-mongodb\\nCopyfrom langchain_mongodb import MongoDBAtlasVectorSearch\\n\\nvector_store = MongoDBAtlasVectorSearch(\\n    embedding=embeddings,\\n    collection=MONGODB_COLLECTION,\\n    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\\n    relevance_score_fn=\"cosine\",\\n)\\nCopypip install -qU langchain-postgres\\nCopyfrom langchain_postgres import PGVector\\n\\nvector_store = PGVector(\\n    embeddings=embeddings,\\n    collection_name=\"my_docs\",\\n    connection=\"postgresql+psycopg://...\",\\n)\\nCopypip install -qU langchain-postgres\\nCopyfrom langchain_postgres import PGEngine, PGVectorStore\\n\\npg_engine = PGEngine.from_connection_string(\\n    url=\"postgresql+psycopg://...\"\\n)\\n\\nvector_store = PGVectorStore.create_sync(\\n    engine=pg_engine,\\n    table_name=\\'test_table\\',\\n    embedding_service=embedding\\n)\\nCopypip install -qU langchain-pinecone\\nCopyfrom langchain_pinecone import PineconeVectorStore\\nfrom pinecone import Pinecone\\n\\npc = Pinecone(api_key=...)\\nindex = pc.Index(index_name)\\n\\nvector_store = PineconeVectorStore(embedding=embeddings, index=index)\\nCopypip install -qU langchain-qdrant\\nCopyfrom qdrant_client.models import Distance, VectorParams\\nfrom langchain_qdrant import QdrantVectorStore\\nfrom qdrant_client import QdrantClient\\n\\nclient = QdrantClient(\":memory:\")\\n\\nvector_size = len(embeddings.embed_query(\"sample text\"))\\n\\nif not client.collection_exists(\"test\"):\\n    client.create_collection(\\n        collection_name=\"test\",\\n        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\\n    )\\nvector_store = QdrantVectorStore(\\n    client=client,\\n    collection_name=\"test\",\\n    embedding=embeddings,\\n)\\n\\n\\u200b1. Indexing\\nThis section is an abbreviated version of the content in the semantic search tutorial.If your data is already indexed and available for search (i.e., you have a function to execute a search), or if youâ€™re comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation.\\nIndexing commonly works as follows:\\n\\nLoad: First we need to load our data. This is done with Document Loaders.\\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and wonâ€™t fit in a modelâ€™s finite context window.\\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\\n\\n\\n\\u200bLoading documents\\nWe need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects.\\nIn this case weâ€™ll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class â€œpost-contentâ€, â€œpost-titleâ€, or â€œpost-headerâ€ are relevant, so weâ€™ll remove all others.\\nCopyimport bs4\\nfrom langchain_community.document_loaders import WebBaseLoader\\n\\n# Only keep post title, headers, and content from the full HTML.\\nbs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs={\"parse_only\": bs4_strainer},\\n)\\ndocs = loader.load()\\n\\nassert len(docs) == 1\\nprint(f\"Total characters: {len(docs[0].page_content)}\")\\n\\nCopyTotal characters: 43131\\n\\nCopyprint(docs[0].page_content[:500])\\n\\nCopy      LLM Powered Autonomous Agents\\n\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn\\n\\nGo deeper\\nDocumentLoader: Object that loads data from a source as list of Documents.\\n\\nIntegrations: 160+ integrations to choose from.\\nBaseLoader: API reference for the base interface.\\n\\n\\u200bSplitting documents\\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\\nTo handle this weâ€™ll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\\nAs in the semantic search tutorial, we use a RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\\nCopyfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=1000,  # chunk size (characters)\\n    chunk_overlap=200,  # chunk overlap (characters)\\n    add_start_index=True,  # track index in original document\\n)\\nall_splits = text_splitter.split_documents(docs)\\n\\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\\n\\nCopySplit blog post into 66 sub-documents.\\n\\nGo deeper\\nTextSplitter: Object that splits a list of Document objects into smaller\\nchunks for storage and retrieval.\\n\\nIntegrations\\nInterface: API reference for the base interface.\\n\\n\\u200bStoring documents\\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.\\nCopydocument_ids = vector_store.add_documents(documents=all_splits)\\n\\nprint(document_ids[:3])\\n\\nCopy[\\'07c18af6-ad58-479a-bfb1-d508033f9c64\\', \\'9000bf8e-1993-446f-8d4d-f4e507ba4b8f\\', \\'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6\\']\\n\\nGo deeper\\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings.\\n\\nIntegrations: 30+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nVectorStore: Wrapper around a vector database, used for storing and querying embeddings.\\n\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nThis completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\\n\\u200b2. Retrieval and Generation\\nRAG applications commonly work as follows:\\n\\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\nGenerate: A model produces an answer using a prompt that includes both the question with the retrieved data\\n\\n\\nNow letâ€™s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\\nWe will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n\\u200bRAG agents\\nOne formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:\\nCopyfrom langchain.tools import tool\\n\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\nHere we use the tool decorator to configure the tool to attach raw documents as artifacts to each ToolMessage. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\\nRetrieval tools are not limited to a single string query argument, as in the above example. You can\\nforce the LLM to specify additional search parameters by adding argumentsâ€” for example, a category:Copyfrom typing import Literal\\n\\ndef retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\\n\\nGiven our tool, we can construct the agent:\\nCopyfrom langchain.agents import create_agent\\n\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\n\\nLetâ€™s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\\nCopyquery = (\\n    \"What is the standard method for Task Decomposition?\\\\n\\\\n\"\\n    \"Once you get the answer, look up common extensions of that method.\"\\n)\\n\\nfor event in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    event[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is the standard method for Task Decomposition?\\n\\nOnce you get the answer, look up common extensions of that method.\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\\n Call ID: call_d6AVxICMPQYwAKj9lgH4E337\\n  Args:\\n    query: standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\\n Call ID: call_0dbMOw7266jvETbXWn4JqWpR\\n  Args:\\n    query: common extensions of the standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\\n\\nNote that the agent:\\n\\nGenerates a query to search for a standard method for task decomposition;\\nReceiving the answer, generates a second query to search for common extensions of it;\\nHaving received all necessary context, answers the question.\\n\\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\\nYou can add a deeper level of control and customization using the LangGraph framework directlyâ€” for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraphâ€™s Agentic RAG tutorial for more advanced formulations.\\n\\u200bRAG chains\\nIn the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\\nâœ… Benefitsâš ï¸ DrawbacksSearch only when needed â€“ The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.Two inference calls â€“ When a search is performed, it requires one call to generate the query and another to produce the final response.Contextual search queries â€“ By treating search as a tool with a query input, the LLM crafts its own queries that incorporate conversational context.Reduced control â€“ The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.Multiple searches allowed â€“ The LLM can execute several searches in support of a single user query.\\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\\nIn this approach we no longer call the model in a loop, but instead make a single pass.\\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\\nCopyfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\\n\\n@dynamic_prompt\\ndef prompt_with_context(request: ModelRequest) -> str:\\n    \"\"\"Inject context into state messages.\"\"\"\\n    last_query = request.state[\"messages\"][-1].text\\n    retrieved_docs = vector_store.similarity_search(last_query)\\n\\n    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n    system_message = (\\n        \"You are a helpful assistant. Use the following context in your response:\"\\n        f\"\\\\n\\\\n{docs_content}\"\\n    )\\n\\n    return system_message\\n\\n\\nagent = create_agent(model, tools=[], middleware=[prompt_with_context])\\n\\nLetâ€™s try this out:\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\n\\nTask decomposition is...\\n\\nIn the LangSmith trace we can see the retrieved context incorporated into the model prompt.\\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\\nReturning source documentsThe above RAG chain incorporates retrieved context into a single system message for that run.As in the agentic RAG formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\\nAdding a key to the state to store the retrieved documents\\nAdding a new node via a pre-model hook to populate that key (as well as inject the context).\\nCopyfrom typing import Any\\nfrom langchain_core.documents import Document\\nfrom langchain.agents.middleware import AgentMiddleware, AgentState\\n\\n\\nclass State(AgentState):\\n    context: list[Document]\\n\\n\\nclass RetrieveDocumentsMiddleware(AgentMiddleware[State]):\\n    state_schema = State\\n\\n    def before_model(self, state: AgentState) -> dict[str, Any] | None:\\n        last_message = state[\"messages\"][-1]\\n        retrieved_docs = vector_store.similarity_search(last_message.text)\\n\\n        docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n        augmented_message_content = (\\n            f\"{last_message.text}\\\\n\\\\n\"\\n            \"Use the following context to answer the query:\\\\n\"\\n            f\"{docs_content}\"\\n        )\\n        return {\\n            \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\\n            \"context\": retrieved_docs,\\n        }\\n\\n\\nagent = create_agent(\\n    model,\\n    tools=[],\\n    middleware=[RetrieveDocumentsMiddleware()],\\n)\\n\\n\\u200bNext steps\\nNow that weâ€™ve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:\\n\\nStream tokens and other information for responsive user experiences\\nAdd conversational memory to support multi-turn interactions\\nAdd long-term memory to support memory across conversational threads\\nAdd structured responses\\nDeploy your application with LangSmith Deployment\\n\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoBuild a semantic search engine with LangChainPreviousBuild a SQL agentNextâŒ˜IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')],\n",
              " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/qa_chat_history/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a RAG agent with LangChain - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentVoice agentMulti-agentLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and GenerationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy page\\u200bOverview\\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n\\u200bConcepts\\nWe will cover the following concepts:\\n\\n\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\\n\\n\\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\\n\\n\\nOnce weâ€™ve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.If your data is already available for search (i.e., you have a function to execute a search), or youâ€™re comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\\n\\u200bPreview\\nIn this guide weâ€™ll build an app that answers questions about the websiteâ€™s content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\\nExpand for full code snippetCopyimport bs4\\nfrom langchain.agents import AgentState, create_agent\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain.messages import MessageLikeRepresentation\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\n# Load and chunk contents of the blog\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs=dict(\\n        parse_only=bs4.SoupStrainer(\\n            class_=(\"post-content\", \"post-title\", \"post-header\")\\n        )\\n    ),\\n)\\ndocs = loader.load()\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\nall_splits = text_splitter.split_documents(docs)\\n\\n# Index chunks\\n_ = vector_store.add_documents(documents=all_splits)\\n\\n# Construct a tool for retrieving context\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\\n Call ID: call_xTkJr8njRY0geNz43ZvGkX0R\\n  Args:\\n    query: task decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done by...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nTask decomposition refers to...\\nCheck out the LangSmith trace.\\n\\u200bSetup\\n\\u200bInstallation\\nThis tutorial requires these langchain dependencies:\\npipuvCopypip install langchain langchain-text-splitters langchain-community bs4\\n\\nFor more details, see our Installation guide.\\n\\u200bLangSmith\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\nOr, set them in Python:\\nCopyimport getpass\\nimport os\\n\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\n\\n\\u200bComponents\\nWe will need to select three components from LangChainâ€™s suite of integrations.\\nSelect a chat model:\\n OpenAI Anthropic Azure Google Gemini AWS Bedrock HuggingFaceðŸ‘‰ Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\nðŸ‘‰ Read the Anthropic chat model integration docsCopypip install -U \"langchain[anthropic]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"claude-sonnet-4-5-20250929\")\\nðŸ‘‰ Read the Azure chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\\n\\nmodel = init_chat_model(\\n    \"azure_openai:gpt-4.1\",\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n)\\nðŸ‘‰ Read the Google GenAI chat model integration docsCopypip install -U \"langchain[google-genai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\\n\\nmodel = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\\nðŸ‘‰ Read the AWS Bedrock chat model integration docsCopypip install -U \"langchain[aws]\"\\ninit_chat_modelModel ClassCopyfrom langchain.chat_models import init_chat_model\\n\\n# Follow the steps here to configure your credentials:\\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\\n\\nmodel = init_chat_model(\\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\\n    model_provider=\"bedrock_converse\",\\n)\\nðŸ‘‰ Read the HuggingFace chat model integration docsCopypip install -U \"langchain[huggingface]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\\n\\nmodel = init_chat_model(\\n    \"microsoft/Phi-3-mini-4k-instruct\",\\n    model_provider=\"huggingface\",\\n    temperature=0.7,\\n    max_tokens=1024,\\n)\\n\\nSelect an embeddings model:\\n OpenAI Azure Google Gemini Google Vertex AWS HuggingFace Ollama Cohere MistralAI Nomic NVIDIA Voyage AI IBM watsonx Fake IsaacusCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\\n\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\nCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\\n    os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for Azure: \")\\n\\nfrom langchain_openai import AzureOpenAIEmbeddings\\n\\nembeddings = AzureOpenAIEmbeddings(\\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\\n)\\nCopypip install -qU langchain-google-genai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"GOOGLE_API_KEY\"):\\n    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\\n\\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\\n\\nembeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\\nCopypip install -qU langchain-google-vertexai\\nCopyfrom langchain_google_vertexai import VertexAIEmbeddings\\n\\nembeddings = VertexAIEmbeddings(model=\"text-embedding-005\")\\nCopypip install -qU langchain-aws\\nCopyfrom langchain_aws import BedrockEmbeddings\\n\\nembeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\")\\nCopypip install -qU langchain-huggingface\\nCopyfrom langchain_huggingface import HuggingFaceEmbeddings\\n\\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\\nCopypip install -qU langchain-ollama\\nCopyfrom langchain_ollama import OllamaEmbeddings\\n\\nembeddings = OllamaEmbeddings(model=\"llama3\")\\nCopypip install -qU langchain-cohere\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"COHERE_API_KEY\"):\\n    os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter API key for Cohere: \")\\n\\nfrom langchain_cohere import CohereEmbeddings\\n\\nembeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\\nCopypip install -qU langchain-mistralai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"MISTRALAI_API_KEY\"):\\n    os.environ[\"MISTRALAI_API_KEY\"] = getpass.getpass(\"Enter API key for MistralAI: \")\\n\\nfrom langchain_mistralai import MistralAIEmbeddings\\n\\nembeddings = MistralAIEmbeddings(model=\"mistral-embed\")\\nCopypip install -qU langchain-nomic\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"NOMIC_API_KEY\"):\\n    os.environ[\"NOMIC_API_KEY\"] = getpass.getpass(\"Enter API key for Nomic: \")\\n\\nfrom langchain_nomic import NomicEmbeddings\\n\\nembeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\\nCopypip install -qU langchain-nvidia-ai-endpoints\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"NVIDIA_API_KEY\"):\\n    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")\\n\\nfrom langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\\n\\nembeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\")\\nCopypip install -qU langchain-voyageai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"VOYAGE_API_KEY\"):\\n    os.environ[\"VOYAGE_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\\n\\nfrom langchain-voyageai import VoyageAIEmbeddings\\n\\nembeddings = VoyageAIEmbeddings(model=\"voyage-3\")\\nCopypip install -qU langchain-ibm\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"WATSONX_APIKEY\"):\\n    os.environ[\"WATSONX_APIKEY\"] = getpass.getpass(\"Enter API key for IBM watsonx: \")\\n\\nfrom langchain_ibm import WatsonxEmbeddings\\n\\nembeddings = WatsonxEmbeddings(\\n    model_id=\"ibm/slate-125m-english-rtrvr\",\\n    url=\"https://us-south.ml.cloud.ibm.com\",\\n    project_id=\"<WATSONX PROJECT_ID>\",\\n)\\nCopypip install -qU langchain-core\\nCopyfrom langchain_core.embeddings import DeterministicFakeEmbedding\\n\\nembeddings = DeterministicFakeEmbedding(size=4096)\\nCopypip install -qU langchain-isaacus\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"ISAACUS_API_KEY\"):\\nos.environ[\"ISAACUS_API_KEY\"] = getpass.getpass(\"Enter API key for Isaacus: \")\\n\\nfrom langchain_isaacus import IsaacusEmbeddings\\n\\nembeddings = IsaacusEmbeddings(model=\"kanon-2-embedder\")\\n\\nSelect a vector store:\\n In-memory Amazon OpenSearch AstraDB Chroma FAISS Milvus MongoDB PGVector PGVectorStore Pinecone QdrantCopypip install -U \"langchain-core\"\\nCopyfrom langchain_core.vectorstores import InMemoryVectorStore\\n\\nvector_store = InMemoryVectorStore(embeddings)\\nCopypip install -qU  boto3\\nCopyfrom opensearchpy import RequestsHttpConnection\\n\\nservice = \"es\"  # must set the service as \\'es\\'\\nregion = \"us-east-2\"\\ncredentials = boto3.Session(\\n    aws_access_key_id=\"xxxxxx\", aws_secret_access_key=\"xxxxx\"\\n).get_credentials()\\nawsauth = AWS4Auth(\"xxxxx\", \"xxxxxx\", region, service, session_token=credentials.token)\\n\\nvector_store = OpenSearchVectorSearch.from_documents(\\n    docs,\\n    embeddings,\\n    opensearch_url=\"host url\",\\n    http_auth=awsauth,\\n    timeout=300,\\n    use_ssl=True,\\n    verify_certs=True,\\n    connection_class=RequestsHttpConnection,\\n    index_name=\"test-index\",\\n)\\nCopypip install -U \"langchain-astradb\"\\nCopyfrom langchain_astradb import AstraDBVectorStore\\n\\nvector_store = AstraDBVectorStore(\\n    embedding=embeddings,\\n    api_endpoint=ASTRA_DB_API_ENDPOINT,\\n    collection_name=\"astra_vector_langchain\",\\n    token=ASTRA_DB_APPLICATION_TOKEN,\\n    namespace=ASTRA_DB_NAMESPACE,\\n)\\nCopypip install -qU langchain-chroma\\nCopyfrom langchain_chroma import Chroma\\n\\nvector_store = Chroma(\\n    collection_name=\"example_collection\",\\n    embedding_function=embeddings,\\n    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\\n)\\nCopypip install -qU langchain-community faiss-cpu\\nCopyimport faiss\\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\\nfrom langchain_community.vectorstores import FAISS\\n\\nembedding_dim = len(embeddings.embed_query(\"hello world\"))\\nindex = faiss.IndexFlatL2(embedding_dim)\\n\\nvector_store = FAISS(\\n    embedding_function=embeddings,\\n    index=index,\\n    docstore=InMemoryDocstore(),\\n    index_to_docstore_id={},\\n)\\nCopypip install -qU langchain-milvus\\nCopyfrom langchain_milvus import Milvus\\n\\nURI = \"./milvus_example.db\"\\n\\nvector_store = Milvus(\\n    embedding_function=embeddings,\\n    connection_args={\"uri\": URI},\\n    index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\\n)\\nCopypip install -qU langchain-mongodb\\nCopyfrom langchain_mongodb import MongoDBAtlasVectorSearch\\n\\nvector_store = MongoDBAtlasVectorSearch(\\n    embedding=embeddings,\\n    collection=MONGODB_COLLECTION,\\n    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\\n    relevance_score_fn=\"cosine\",\\n)\\nCopypip install -qU langchain-postgres\\nCopyfrom langchain_postgres import PGVector\\n\\nvector_store = PGVector(\\n    embeddings=embeddings,\\n    collection_name=\"my_docs\",\\n    connection=\"postgresql+psycopg://...\",\\n)\\nCopypip install -qU langchain-postgres\\nCopyfrom langchain_postgres import PGEngine, PGVectorStore\\n\\npg_engine = PGEngine.from_connection_string(\\n    url=\"postgresql+psycopg://...\"\\n)\\n\\nvector_store = PGVectorStore.create_sync(\\n    engine=pg_engine,\\n    table_name=\\'test_table\\',\\n    embedding_service=embedding\\n)\\nCopypip install -qU langchain-pinecone\\nCopyfrom langchain_pinecone import PineconeVectorStore\\nfrom pinecone import Pinecone\\n\\npc = Pinecone(api_key=...)\\nindex = pc.Index(index_name)\\n\\nvector_store = PineconeVectorStore(embedding=embeddings, index=index)\\nCopypip install -qU langchain-qdrant\\nCopyfrom qdrant_client.models import Distance, VectorParams\\nfrom langchain_qdrant import QdrantVectorStore\\nfrom qdrant_client import QdrantClient\\n\\nclient = QdrantClient(\":memory:\")\\n\\nvector_size = len(embeddings.embed_query(\"sample text\"))\\n\\nif not client.collection_exists(\"test\"):\\n    client.create_collection(\\n        collection_name=\"test\",\\n        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\\n    )\\nvector_store = QdrantVectorStore(\\n    client=client,\\n    collection_name=\"test\",\\n    embedding=embeddings,\\n)\\n\\n\\u200b1. Indexing\\nThis section is an abbreviated version of the content in the semantic search tutorial.If your data is already indexed and available for search (i.e., you have a function to execute a search), or if youâ€™re comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation.\\nIndexing commonly works as follows:\\n\\nLoad: First we need to load our data. This is done with Document Loaders.\\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and wonâ€™t fit in a modelâ€™s finite context window.\\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\\n\\n\\n\\u200bLoading documents\\nWe need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects.\\nIn this case weâ€™ll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class â€œpost-contentâ€, â€œpost-titleâ€, or â€œpost-headerâ€ are relevant, so weâ€™ll remove all others.\\nCopyimport bs4\\nfrom langchain_community.document_loaders import WebBaseLoader\\n\\n# Only keep post title, headers, and content from the full HTML.\\nbs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs={\"parse_only\": bs4_strainer},\\n)\\ndocs = loader.load()\\n\\nassert len(docs) == 1\\nprint(f\"Total characters: {len(docs[0].page_content)}\")\\n\\nCopyTotal characters: 43131\\n\\nCopyprint(docs[0].page_content[:500])\\n\\nCopy      LLM Powered Autonomous Agents\\n\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn\\n\\nGo deeper\\nDocumentLoader: Object that loads data from a source as list of Documents.\\n\\nIntegrations: 160+ integrations to choose from.\\nBaseLoader: API reference for the base interface.\\n\\n\\u200bSplitting documents\\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\\nTo handle this weâ€™ll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\\nAs in the semantic search tutorial, we use a RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\\nCopyfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=1000,  # chunk size (characters)\\n    chunk_overlap=200,  # chunk overlap (characters)\\n    add_start_index=True,  # track index in original document\\n)\\nall_splits = text_splitter.split_documents(docs)\\n\\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\\n\\nCopySplit blog post into 66 sub-documents.\\n\\nGo deeper\\nTextSplitter: Object that splits a list of Document objects into smaller\\nchunks for storage and retrieval.\\n\\nIntegrations\\nInterface: API reference for the base interface.\\n\\n\\u200bStoring documents\\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.\\nCopydocument_ids = vector_store.add_documents(documents=all_splits)\\n\\nprint(document_ids[:3])\\n\\nCopy[\\'07c18af6-ad58-479a-bfb1-d508033f9c64\\', \\'9000bf8e-1993-446f-8d4d-f4e507ba4b8f\\', \\'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6\\']\\n\\nGo deeper\\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings.\\n\\nIntegrations: 30+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nVectorStore: Wrapper around a vector database, used for storing and querying embeddings.\\n\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nThis completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\\n\\u200b2. Retrieval and Generation\\nRAG applications commonly work as follows:\\n\\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\nGenerate: A model produces an answer using a prompt that includes both the question with the retrieved data\\n\\n\\nNow letâ€™s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\\nWe will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n\\u200bRAG agents\\nOne formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:\\nCopyfrom langchain.tools import tool\\n\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\nHere we use the tool decorator to configure the tool to attach raw documents as artifacts to each ToolMessage. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\\nRetrieval tools are not limited to a single string query argument, as in the above example. You can\\nforce the LLM to specify additional search parameters by adding argumentsâ€” for example, a category:Copyfrom typing import Literal\\n\\ndef retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\\n\\nGiven our tool, we can construct the agent:\\nCopyfrom langchain.agents import create_agent\\n\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\n\\nLetâ€™s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\\nCopyquery = (\\n    \"What is the standard method for Task Decomposition?\\\\n\\\\n\"\\n    \"Once you get the answer, look up common extensions of that method.\"\\n)\\n\\nfor event in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    event[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is the standard method for Task Decomposition?\\n\\nOnce you get the answer, look up common extensions of that method.\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\\n Call ID: call_d6AVxICMPQYwAKj9lgH4E337\\n  Args:\\n    query: standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\\n Call ID: call_0dbMOw7266jvETbXWn4JqWpR\\n  Args:\\n    query: common extensions of the standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\\n\\nNote that the agent:\\n\\nGenerates a query to search for a standard method for task decomposition;\\nReceiving the answer, generates a second query to search for common extensions of it;\\nHaving received all necessary context, answers the question.\\n\\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\\nYou can add a deeper level of control and customization using the LangGraph framework directlyâ€” for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraphâ€™s Agentic RAG tutorial for more advanced formulations.\\n\\u200bRAG chains\\nIn the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\\nâœ… Benefitsâš ï¸ DrawbacksSearch only when needed â€“ The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.Two inference calls â€“ When a search is performed, it requires one call to generate the query and another to produce the final response.Contextual search queries â€“ By treating search as a tool with a query input, the LLM crafts its own queries that incorporate conversational context.Reduced control â€“ The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.Multiple searches allowed â€“ The LLM can execute several searches in support of a single user query.\\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\\nIn this approach we no longer call the model in a loop, but instead make a single pass.\\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\\nCopyfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\\n\\n@dynamic_prompt\\ndef prompt_with_context(request: ModelRequest) -> str:\\n    \"\"\"Inject context into state messages.\"\"\"\\n    last_query = request.state[\"messages\"][-1].text\\n    retrieved_docs = vector_store.similarity_search(last_query)\\n\\n    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n    system_message = (\\n        \"You are a helpful assistant. Use the following context in your response:\"\\n        f\"\\\\n\\\\n{docs_content}\"\\n    )\\n\\n    return system_message\\n\\n\\nagent = create_agent(model, tools=[], middleware=[prompt_with_context])\\n\\nLetâ€™s try this out:\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\n\\nTask decomposition is...\\n\\nIn the LangSmith trace we can see the retrieved context incorporated into the model prompt.\\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\\nReturning source documentsThe above RAG chain incorporates retrieved context into a single system message for that run.As in the agentic RAG formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\\nAdding a key to the state to store the retrieved documents\\nAdding a new node via a pre-model hook to populate that key (as well as inject the context).\\nCopyfrom typing import Any\\nfrom langchain_core.documents import Document\\nfrom langchain.agents.middleware import AgentMiddleware, AgentState\\n\\n\\nclass State(AgentState):\\n    context: list[Document]\\n\\n\\nclass RetrieveDocumentsMiddleware(AgentMiddleware[State]):\\n    state_schema = State\\n\\n    def before_model(self, state: AgentState) -> dict[str, Any] | None:\\n        last_message = state[\"messages\"][-1]\\n        retrieved_docs = vector_store.similarity_search(last_message.text)\\n\\n        docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n        augmented_message_content = (\\n            f\"{last_message.text}\\\\n\\\\n\"\\n            \"Use the following context to answer the query:\\\\n\"\\n            f\"{docs_content}\"\\n        )\\n        return {\\n            \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\\n            \"context\": retrieved_docs,\\n        }\\n\\n\\nagent = create_agent(\\n    model,\\n    tools=[],\\n    middleware=[RetrieveDocumentsMiddleware()],\\n)\\n\\n\\u200bNext steps\\nNow that weâ€™ve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:\\n\\nStream tokens and other information for responsive user experiences\\nAdd conversational memory to support multi-turn interactions\\nAdd long-term memory to support memory across conversational threads\\nAdd structured responses\\nDeploy your application with LangSmith Deployment\\n\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoBuild a semantic search engine with LangChainPreviousBuild a SQL agentNextâŒ˜IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]]"
            ]
          },
          "execution_count": 297,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#document loading\n",
        "urls=[\n",
        "    \"https://langchain-ai.github.io/langgraph/tutorials/introduction/\",\n",
        "    \"https://langchain-ai.github.io/langgraph/tutorials/workflows/\",\n",
        "    \"https://langchain-ai.github.io/langgraph/how-tos/map-reduce/\",\n",
        "    \"https://python.langchain.com/docs/tutorials/\",\n",
        "    \"https://python.langchain.com/docs/tutorials/chatbot/\",\n",
        "    \"https://python.langchain.com/docs/tutorials/qa_chat_history/\"\n",
        "]\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ad2630b",
      "metadata": {
        "id": "1ad2630b"
      },
      "outputs": [],
      "source": [
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=100\n",
        ")\n",
        "\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "## Add alll these text to vectordb\n",
        "\n",
        "vectorstore=FAISS.from_documents(\n",
        "    documents=doc_splits,\n",
        "    embedding=OpenAIEmbeddings()\n",
        ")\n",
        "\n",
        "\n",
        "retriever=vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb2d11fc",
      "metadata": {
        "id": "cb2d11fc",
        "outputId": "232bf8c6-fd39-4980-d7c0-c3d7b5b4c86b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='f6f8bba8-222e-49e9-8742-84e3e919dde9', metadata={'source': 'https://python.langchain.com/docs/tutorials/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a RAG agent with LangChain - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentVoice agentMulti-agentLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and GenerationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy page\\u200bOverview'),\n",
              " Document(id='b0c8706f-11c8-4306-8414-64cff79a858c', metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a RAG agent with LangChain - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentVoice agentMulti-agentLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and GenerationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy page\\u200bOverview'),\n",
              " Document(id='f9928203-2e42-4254-99d9-9e1462ca9352', metadata={'source': 'https://python.langchain.com/docs/tutorials/qa_chat_history/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a RAG agent with LangChain - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentVoice agentMulti-agentLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and GenerationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy page\\u200bOverview'),\n",
              " Document(id='0ba409f5-fa47-4bc6-9a95-bd0ee90d17c0', metadata={'source': 'https://python.langchain.com/docs/tutorials/qa_chat_history/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='For more details, see our Installation guide.\\n\\u200bLangSmith\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\nOr, set them in Python:\\nCopyimport getpass\\nimport os\\n\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()')]"
            ]
          },
          "execution_count": 299,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever.invoke(\"what is langchain\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83de8a84",
      "metadata": {
        "id": "83de8a84",
        "outputId": "5831f639-b5dc-4a19-cde6-7c00acac6f53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'LangChain helps you ship quickly with less code using a pre-built agent architecture and model integrations.See exactly what your agent is doing. What is LangChain ?IBM Technology593K views â€¢ 1 year agoLivePlaylist ()Mix (50+). 26:03. The LangChain Cookbook Part 2 - Beginner Guide To 9 Use CasesGreg Kamradt85K views... LangChain makes it easy to connect LLMs like OpenAIâ€™s GPT-4 Turbo, Claude, Mistral, and Llama to various knowledge sources. Which LLM Does LangChain Use? LangChain is a cutting-edge language learning platform that leverages the power of technology, artificial intelligence, and community collaboration to provide learners with an immersive... What is LangChain . Describe What You Want to Automate. Latenode will turn your prompt into a ready-to-run workflow in seconds. Enter a message.'"
            ]
          },
          "execution_count": 300,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "\n",
        "search_tool = DuckDuckGoSearchRun()\n",
        "search_tool.run(\"what is langchain\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f4ad0be",
      "metadata": {
        "id": "5f4ad0be"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "def query_optimizer(state):\n",
        "    \"\"\"Optimize the user query for better retrieval\"\"\"\n",
        "\n",
        "    print(\"OPTIMIZING QUERY....\")\n",
        "\n",
        "    question = state[\"original_query\"]\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"\n",
        "Rewrite the following query to make it clearer and more specific for information retrieval.\n",
        "Do not change the original meaning.\n",
        "Do not answer the question.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Return only the improved query.\n",
        "\"\"\",\n",
        "        input_variables=[\"question\"]\n",
        "    )\n",
        "\n",
        "    response = llm.invoke(prompt.format(question=question))\n",
        "\n",
        "    return {\n",
        "        \"original_query\": question,\n",
        "        \"optimized_query\": response.content\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb09a023",
      "metadata": {
        "id": "eb09a023"
      },
      "outputs": [],
      "source": [
        "test_state = {\n",
        "    \"original_query\": \"Explain RAG\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6257957e",
      "metadata": {
        "id": "6257957e",
        "outputId": "25367e8a-c6d7-4409-c619-a85068157d80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPTIMIZING QUERY....\n",
            "{'original_query': 'Explain RAG', 'optimized_query': 'Provide a detailed explanation of the term \"RAG,\" including its definition, context, and any relevant applications or examples.'}\n"
          ]
        }
      ],
      "source": [
        "result = query_optimizer(test_state)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a31260f",
      "metadata": {
        "id": "9a31260f",
        "outputId": "e5997fb1-669a-4c88-880c-88e014d412d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'original_query': 'Explain RAG'}"
            ]
          },
          "execution_count": 304,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "364d0a83",
      "metadata": {
        "id": "364d0a83"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "def plan_steps(state):\n",
        "    \"\"\"Decompose the optimized query into sub-queries\"\"\"\n",
        "    print(\"PLANNING STEPS....\")\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"\n",
        "Break the following optimized query into 2â€“3 clear, reasonable steps\n",
        "required to answer it.\n",
        "Do not answer the query.\n",
        "\n",
        "Optimized Query:\n",
        "{optimized_query}\n",
        "\n",
        "Return each step on a new line.\n",
        "\"\"\",\n",
        "        input_variables=[\"optimized_query\"]\n",
        "    )\n",
        "\n",
        "    optimized_query = state[\"optimized_query\"]\n",
        "\n",
        "    response = llm.invoke(\n",
        "        prompt.format(optimized_query=optimized_query)\n",
        "    )\n",
        "\n",
        "    subqs = [line.strip(\"- \").strip() for line in response.content.split(\"\\n\") if line.strip()]\n",
        "\n",
        "    return {\"sub_queries\": subqs}\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c2ac9bf",
      "metadata": {
        "id": "4c2ac9bf"
      },
      "outputs": [],
      "source": [
        "def retriever_per_step(state:dict) -> dict:\n",
        "    \"\"\"Retrieve documents for each sub query\"\"\"\n",
        "    print(\"RETRIEVING DOCUMENTS....\")\n",
        "    all_docs= []\n",
        "    sub_queries = state[\"sub_queries\"]\n",
        "    for query in sub_queries:\n",
        "        docs = retriever.invoke(query)\n",
        "        all_docs.extend(docs)\n",
        "        return {\"retrieved_docs\": all_docs}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3788e95d",
      "metadata": {
        "id": "3788e95d"
      },
      "outputs": [],
      "source": [
        "#def generate response\n",
        "def generate_response(state):\n",
        "    \"\"\"generate answer based on retrieved documents\"\"\"\n",
        "    print(\"GENERATING RESPONSE....\")\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in state[\"retrieved_docs\"]])\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"You are answering a complex question using retrieved documents.\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Relevant Information:\n",
        "{context}\n",
        "\n",
        "Now synthesize a clear, accurate, and well-reasoned final answer.\n",
        "\"\"\",\n",
        "        input_variables=[\"question\", \"context\"]\n",
        "    )\n",
        "\n",
        "    response = llm.invoke(prompt.format(question=state[\"original_query\"],context=context))\n",
        "    return {\"answer\": response.content.strip()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5027173",
      "metadata": {
        "id": "f5027173"
      },
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "def evaluate_response(state):\n",
        "    print(\"EVALUATING RESPONSE....\")\n",
        "\n",
        "    class Grade(BaseModel):\n",
        "        score: Literal[\"yes\", \"no\"]\n",
        "\n",
        "    model = llm.with_structured_output(Grade)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Question:\n",
        "{state[\"original_query\"]}\n",
        "\n",
        "Answer:\n",
        "{state[\"answer\"]}\n",
        "\n",
        "Is the answer correct and complete?\n",
        "Respond with yes or no.\n",
        "\"\"\"\n",
        "\n",
        "    response = model.invoke(prompt)\n",
        "\n",
        "    retry = state.get(\"retry_count\", 0) + 1\n",
        "    grade = \"approve\" if response.score == \"yes\" else \"rewrite\"\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"grade\": grade,\n",
        "        \"retry_count\": retry\n",
        "    }\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30132ad1",
      "metadata": {
        "id": "30132ad1"
      },
      "outputs": [],
      "source": [
        "def route_response(state) -> Literal[\"finalize\", \"rewrite\"]:\n",
        "    print(\"ROUTING RESPONSE....\")\n",
        "\n",
        "    if state[\"grade\"] == \"approve\":\n",
        "        return \"finalize\"\n",
        "\n",
        "    if state[\"retry_count\"] >= 3:\n",
        "        print(\"MAX RETRIES â€” FINALIZING BEST ANSWER\")\n",
        "        return \"finalize\"\n",
        "\n",
        "    return \"rewrite\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3dfa2b1",
      "metadata": {
        "id": "b3dfa2b1"
      },
      "outputs": [],
      "source": [
        "def rewrite(state):\n",
        "    \"\"\"Rewrite the user query to improve document retrieval\"\"\"\n",
        "    print(\"REWRITING QUERY....\")\n",
        "    question = state[\"original_query\"]\n",
        "    prompt = PromptTemplate(\n",
        "        template=f\"\"\" Look at the input and try to reason about the underlying semantic intent / meaning. \\n\n",
        "    Here is the initial question:\n",
        "    \\n ------- \\n\n",
        "    {question}\n",
        "    \\n ------- \\n\n",
        "    Rewrite the question to make it more specific and clear for information retrieval.\n",
        "    Do not change the original meaning.\n",
        "    Do not answer the question.\n",
        "    \"\"\"\n",
        "    )\n",
        "    response = llm.invoke(prompt.format(question=question))\n",
        "    return {\"rewritten_query\": response.content.strip()}\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee7b0199",
      "metadata": {
        "id": "ee7b0199"
      },
      "outputs": [],
      "source": [
        "def finalize(state):\n",
        "    print(\"FINALIZING ANSWER....\")\n",
        "    return {\"answer\": state[\"answer\"]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab913109",
      "metadata": {
        "id": "ab913109",
        "outputId": "40f59975-28aa-4d58-e0f7-c08aef6989d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x24b615d28e0>"
            ]
          },
          "execution_count": 321,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "workflow = StateGraph(RAGState)\n",
        "\n",
        "# Nodes\n",
        "workflow.add_node(\"query_optimizer\", query_optimizer)\n",
        "workflow.add_node(\"plan_steps\", plan_steps)\n",
        "workflow.add_node(\"retrieval\", retriever_per_step)\n",
        "workflow.add_node(\"generate_response\", generate_response)\n",
        "workflow.add_node(\"evaluate_response\", evaluate_response)\n",
        "workflow.add_node(\"rewrite\", rewrite)\n",
        "workflow.add_node(\"finalize\", finalize)\n",
        "\n",
        "# Linear flow\n",
        "workflow.add_edge(START, \"query_optimizer\")\n",
        "workflow.add_edge(\"query_optimizer\", \"plan_steps\")\n",
        "workflow.add_edge(\"plan_steps\", \"retrieval\")\n",
        "workflow.add_edge(\"retrieval\", \"generate_response\")\n",
        "workflow.add_edge(\"generate_response\", \"evaluate_response\")\n",
        "\n",
        "# âœ… Conditional routing (router is NOT a node)\n",
        "workflow.add_conditional_edges(\n",
        "    \"evaluate_response\",\n",
        "    route_response,\n",
        "    {\n",
        "        \"finalize\": \"finalize\",\n",
        "        \"rewrite\": \"rewrite\",\n",
        "    }\n",
        ")\n",
        "\n",
        "# Loop\n",
        "workflow.add_edge(\"rewrite\", \"query_optimizer\")\n",
        "workflow.add_edge(\"finalize\", END)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be5412f6",
      "metadata": {
        "id": "be5412f6",
        "outputId": "211bf176-e57d-4271-c0d7-32c051801c96"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<langgraph.graph.state.CompiledStateGraph object at 0x0000024B5D5FD150>"
            ]
          },
          "execution_count": 322,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_builder=workflow.compile()\n",
        "graph_builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7107e7ff",
      "metadata": {
        "id": "7107e7ff",
        "outputId": "a15adf59-68c5-4410-f545-2fc9075476d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPTIMIZING QUERY....\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PLANNING STEPS....\n",
            "RETRIEVING DOCUMENTS....\n",
            "GENERATING RESPONSE....\n",
            "EVALUATING RESPONSE....\n",
            "ROUTING RESPONSE....\n",
            "FINALIZING ANSWER....\n",
            "\n",
            "===== FINAL STATE =====\n",
            "\n",
            "original_query: explain  working of langgraph\n",
            "\n",
            "optimized_query: Provide a detailed explanation of how LangGraph functions and its key components.\n",
            "\n",
            "sub_queries: ['1. Research and gather information about LangGraph, including its purpose, functionality, and applications in various contexts.', '2. Identify and describe the key components of LangGraph, explaining their roles and how they interact within the system.', '3. Compile the gathered information into a coherent and detailed explanation that outlines how LangGraph functions as a whole.']\n",
            "\n",
            "retrieved_docs: [Document(id='85ab2273-d30f-413b-9d3d-aa7cb7c01ddc', metadata={'source': 'https://python.langchain.com/docs/tutorials/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='We can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\\nYou can add a deeper level of control and customization using the LangGraph framework directlyâ€” for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraphâ€™s Agentic RAG tutorial for more advanced formulations.\\n\\u200bRAG chains\\nIn the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:'), Document(id='3bc2f1ab-d1de-471c-a391-a2dc236636b9', metadata={'source': 'https://python.langchain.com/docs/tutorials/qa_chat_history/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='We can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\\nYou can add a deeper level of control and customization using the LangGraph framework directlyâ€” for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraphâ€™s Agentic RAG tutorial for more advanced formulations.\\n\\u200bRAG chains\\nIn the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:'), Document(id='ec168c0b-27a9-4307-9852-8e8c7383a55a', metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='We can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\\nYou can add a deeper level of control and customization using the LangGraph framework directlyâ€” for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraphâ€™s Agentic RAG tutorial for more advanced formulations.\\n\\u200bRAG chains\\nIn the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:'), Document(id='f6f8bba8-222e-49e9-8742-84e3e919dde9', metadata={'source': 'https://python.langchain.com/docs/tutorials/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a RAG agent with LangChain - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentVoice agentMulti-agentLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and GenerationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy page\\u200bOverview')]\n",
            "\n",
            "answer: LangGraph is a framework designed to enhance the functionality and customization of retrieval-augmented generation (RAG) systems, particularly in the context of language models (LLMs). Here's a breakdown of how it works:\n",
            "\n",
            "1. **Integration with LangSmith**: LangGraph allows users to visualize the entire sequence of operations performed during a query, including latency and other metadata, through the LangSmith trace. This transparency helps in understanding the performance and efficiency of the system.\n",
            "\n",
            "2. **Customization and Control**: One of the key features of LangGraph is its ability to provide deeper control over the RAG process. Users can directly manipulate the framework to add specific steps that can improve the relevance of retrieved documents. For instance, you can implement mechanisms to grade the relevance of documents based on certain criteria or rewrite search queries to better align with user intent.\n",
            "\n",
            "3. **Agentic RAG Formulation**: LangGraph supports an agentic RAG formulation, where the LLM is empowered to make decisions about when to generate tool calls to assist in answering user queries. This flexibility allows for a more dynamic interaction, enabling the model to adapt its responses based on the context of the query. However, this approach does come with trade-offs, such as the potential for increased complexity in managing the decision-making process.\n",
            "\n",
            "4. **Building RAG Agents**: LangGraph is part of a broader ecosystem that includes tools and documentation for building RAG agents. This includes indexing documents, retrieving relevant information, and generating responses based on the retrieved data. The framework is designed to facilitate the development of sophisticated applications that leverage the strengths of LLMs in conjunction with external data sources.\n",
            "\n",
            "5. **Advanced Tutorials**: For users looking to delve deeper into the capabilities of LangGraph, there are tutorials available, such as the Agentic RAG tutorial, which provide guidance on implementing more advanced configurations and functionalities.\n",
            "\n",
            "In summary, LangGraph enhances the RAG process by offering customization options, transparency in operations, and the ability for LLMs to autonomously generate tool calls, all of which contribute to more effective and relevant responses to user queries.\n",
            "\n",
            "grade: approve\n",
            "\n",
            "retry_count: 1\n"
          ]
        }
      ],
      "source": [
        "query = \"explain  working of langgraph\"\n",
        "state = RAGState(original_query=query)\n",
        "result = graph_builder.invoke(state)\n",
        "\n",
        "print(\"\\n===== FINAL STATE =====\")\n",
        "for key, value in result.items():\n",
        "    print(f\"\\n{key}: {value}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}